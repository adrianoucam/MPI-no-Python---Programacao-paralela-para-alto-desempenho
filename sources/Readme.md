
<b>Descricao de programas exemplo usando MPI no python</b>
<br>
Programa de aproxima√ß√£o de œÄ (pi) usando Allreduce, com explica√ß√£o para uso academico.<br>
<br>
Pense que queremos descobrir o tamanho da pizza (pi). Cada crian√ßa (processo) mede um pedacinho e no final somamos tudo. O Allreduce √© como juntar todas as medidas em uma s√≥ resposta que todos recebem.<br>
mpiexec -n 4 python mpi_pi_criancas.py<br>
mpiexec -n 4 python3 mpi_pi_criancas.py<br>
[Rank 0] valor aproximado de pi: 3.1415926535897936<br>
[Rank 0] erro absoluto em relacao a math.pi: 4.441e-16<br>
[Rank 2] valor aproximado de pi: 3.1415926535897936<br>
[Rank 3] valor aproximado de pi: 3.1415926535897936<br>
[Rank 1] valor aproximado de pi: 3.1415926535897936<br>
<br>
<br>
Bolinhas distribuidas entre o rank0 e rank1<br>
<br>
mpiexec -n 4 python3 mpi_bolinhas.py (mpirun -n 4 python3 mpi_bolinhas.py no linux)<br>
[Rank 3] Estou s√≥ observando a troca de bolinhas.<br>
[Rank 2] Estou s√≥ observando a troca de bolinhas.<br>
[Rank 0] Enviei 10 bolinha(s) para o Rank 1.<br>
[Rank 1] Recebi 10 bolinha(s). De quem? Rank 0 | Etiqueta: 0<br>
[Rank 1] As bolinhas s√ío: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]<br>
<br>
<br>
mpi_scatter<br>
Ideia: o L√≠der (rank 0) tem uma fila de figurinhas (um vetor grand√£o). Com o Scatter, ele divide igualmente e entrega um pacotinho de TAM_VET figurinhas para cada crian√ßa (cada processo), inclusive para ele mesmo.<br>
<br>
mpiexec -n 4 python3 mpi_scatter_criancas.py<br>
Processo 1 recebeu: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]<br>
Processo 2 recebeu: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]<br>
Processo 0 recebeu: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]<br>
Processo 3 recebeu: [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]<br>
<br>
<br>
wtime para avaliar o tempo gasto nos processos<br>
mpiexec -n 8 python3 mpi_wtime_criancas.py --n 50000<br>
[Rank 2] Gastos 0.004514 s para calcular a = 49999 com precisao 6.984e-08 s<br>
[Rank 1] Gastos 0.004384 s para calcular a = 49999 com precisao 6.984e-08 s<br>
[Rank 5] Gastos 0.004365 s para calcular a = 49999 com precisao 6.984e-08 s<br>
[Rank 3] Gastos 0.004053 s para calcular a = 49999 com precisao 6.984e-08 s<br>
[Rank 7] Gastos 0.004564 s para calcular a = 49999 com precisao 6.984e-08 s<br>
[Rank 0] Gastos 0.004747 s para calcular a = 49999 com precisao 6.984e-08 s<br>
[Rank 4] Gastos 0.003916 s para calcular a = 49999 com precisao 6.984e-08 s<br>
[Rank 6] Gastos 0.003573 s para calcular a = 49999 com precisao 6.984e-08 s<br>
<br>
wtime + barrier para uma comparacao justa<br>
mpiexec -n 2 python3 mpi_wtime_criancas_barrier.py --n 500000<br>
[Rank 1] Tempo = 0.044915 s | a = 499999 | precisao = 6.984e-08 s<br>
[Rank 0] Tempo = 0.047111 s | a = 499999 | precisao = 6.984e-08 s<br>
<br>
<br>
MPI SEND + MPI GETCOUNT<br>
Ideia: a Crian√ßa 0 escolhe uma quantidade aleat√≥ria de bolinhas (n√∫meros) e envia para a Crian√ßa 1. A Crian√ßa 1 recebe at√© um m√°ximo de 100 e usa o status da mensagem para descobrir quantas realmente chegaram, al√©m de quem enviou e qual foi a etiqueta (tag).<br>
<br>
mpiexec -n 8 python3 mpi_aleatorio_criancas.py<br>
[Rank 7] Sem papel nesta brincadeira.<br>
[Rank 2] Sem papel nesta brincadeira.<br>
[Rank 4] Sem papel nesta brincadeira.<br>
[Rank 3] Sem papel nesta brincadeira.<br>
[Rank 6] Sem papel nesta brincadeira.<br>
[Rank 5] Sem papel nesta brincadeira.<br>
[Rank 0] Enviei 58 numero(s) para o Rank 1 (tag=42).<br>
[Rank 1] Recebi 58 numero(s). Origem = 0, tag = 42.<br>
[Rank 1] Valores: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, '...']<br>
<br>
<br>
MPI SSEND
Ideia: cada crian√ßa tem um n√∫mero. Em rodadas de ‚Äúdobrar a dist√¢ncia‚Äù (1, depois 2, depois 4, ‚Ä¶), ela troca n√∫meros com uma parceira.<br>
Desta vez usamos envio s√≠ncrono (Ssend): √© como falar cara a cara ‚Äî quem fala s√≥ segue em frente quando tem certeza de que a outra j√° est√° ouvindo (o Recv j√° foi postado). Para n√£o travar, metade das crian√ßas envia primeiro e a outra metade recebe primeiro em cada rodada<br>
<br>
mpiexec -n 4 python3 mpi_sincrona_criancas.py<br>
Rank 2: meu_valor = 8, reducao = 12<br>
Rank 0: meu_valor = 0, reducao = 12<br>
Rank 1: meu_valor = 4, reducao = 12<br>
Rank 3: meu_valor = 12, reducao = 12<br>

<br>
<br>
MPI.Datatype.Create_struct com MPI_Botton, BCast
exemplo com tipo derivado vector e broadcast de uma coluna, explicada de forma academica.<br>
<br>
Ideia: pense numa tabela 4√ó4 de n√∫meros. O L√≠der (rank 0) quer ‚Äúgritar‚Äù para todos os outros a 2¬™ coluna da tabela. Como essa coluna est√° ‚Äúespalhada‚Äù na mem√≥ria (n√£o √© um bloco cont√≠nuo), usamos um tipo derivado ‚Äúvetor‚Äù para dizer ao MPI: ‚Äúpegue 1 n√∫mero por linha, pulando de 4 em 4‚Äù.<br>
<br>
No C, voc√™ passa &matriz[0][1] (um ponteiro para o 1¬∫ elemento da coluna) e o datatype vector faz os ‚Äúsaltos‚Äù. Em mpi4py, para replicar isso direitinho, a forma robusta √©:<br>
criar o datatype em bytes (Create_hvector),<br>
ancor√°-lo no endere√ßo absoluto do primeiro elemento da coluna com um Create_struct,<br>
e transmitir usando MPI.BOTTOM no root.<br>
<br>
mpiexec -n 4 python3 mpi_bcast_coluna_vector.py<br>
Processo 1 - vetor[0] = 2.0 vetor[1] = 6.0 vetor[2] = 10.0 vetor[3] = 14.0<br>
Processo 2 - vetor[0] = 2.0 vetor[1] = 6.0 vetor[2] = 10.0 vetor[3] = 14.0<br>
Processo 3 - vetor[0] = 2.0 vetor[1] = 6.0 vetor[2] = 10.0 vetor[3] = 14.0<br>


<br>
mpi_isend, explicada de forma academica.<br>
<br>
Ideia: cada crian√ßa tem um n√∫mero. Em rodadas de ‚Äúdobrar a dist√¢ncia‚Äù (1, depois 2, depois 4, ‚Ä¶), ela troca seu n√∫mero com uma parceira. Usamos mensageiros r√°pidos (envio/recebimento n√£o bloqueante Isend/Irecv) ‚Äî eles saem correndo enquanto a crian√ßa pode fazer outras coisas; no fim chamamos Wait para garantir que a entrega chegou. Depois de cada troca, a crian√ßa guarda o maior n√∫mero. No final, todas ficam com o mesmo maior n√∫mero.<br>
<br>
mpiexec -n 8 python3 mpi_isend_criancas.py<br>
Rank 4: meu_valor = 32, reducao = 56<br>
Rank 0: meu_valor = 0, reducao = 56<br>
Rank 6: meu_valor = 48, reducao = 56<br>
Rank 2: meu_valor = 16, reducao = 56<br>
Rank 7: meu_valor = 56, reducao = 56<br>
Rank 3: meu_valor = 24, reducao = 56<br>
Rank 5: meu_valor = 40, reducao = 56<br>
Rank 1: meu_valor = 8, reducao = 56<br>
<br>
<br>
mpi_maxloc, explicada de forma academica.<br>
<br>
Ideia: temos 10 ‚Äúcartas‚Äù (posi√ß√µes 0..9). Cada crian√ßa (processo) tem um valor para cada carta. As crian√ßas de n√∫mero par colocam 50.0 na carta com o seu pr√≥prio n√∫mero (se couber em 0..9). A turma quer saber, para cada carta, qual foi o maior valor e em qual crian√ßa ele apareceu ‚Äî isso √© o que o MAXLOC faz.<br>
<br>
Como em mpi4py o uso direto de MPI_MAXLOC com structs pode ser chato, vamos emular o comportamento com duas coletivas:<br>
<br>
Allreduce (MAX) para achar o maior valor de cada carta.<br>
<br>
Reduce (MIN) dos ranks candidatos (quem bateu o m√°ximo naquela carta); quem n√£o bateu manda ‚Äú‚àû‚Äù. Assim, em caso de empate, fica o menor rank (igual ao MAXLOC).<br>
<br>
mpiexec -n 6 python3 mpi_maxloc_criancas.py<br>
Posicao  0: Resultado = 50.0  Processo = 0<br>
Posicao  1: Resultado =  1.0  Processo = 0<br>
Posicao  2: Resultado = 50.0  Processo = 2<br>
Posicao  3: Resultado =  3.0  Processo = 0<br>
Posicao  4: Resultado = 50.0  Processo = 4<br>
Posicao  5: Resultado =  5.0  Processo = 0<br>
Posicao  6: Resultado =  6.0  Processo = 0<br>
Posicao  7: Resultado =  7.0  Processo = 0<br>
Posicao  8: Resultado =  8.0  Processo = 0<br>
Posicao  9: Resultado =  9.0  Processo = 0<br>
<br>
Para cada posi√ß√£o (0..9), o Resultado √© o maior valor alcan√ßado naquela posi√ß√£o entre todos.<br>
<br>
Processo mostra quem (qual rank) bateu esse m√°ximo; se deu empate, fica o menor rank <br>

<BR>
mpi_gather, explicada de forma academica:<br>
<br>
Pense que cada crian√ßa (processo) tem um bloquinho com 10 n√∫meros. Cada crian√ßa preenche seu bloquinho com o seu n√∫mero (o <br>rank). Depois, o L√≠der (rank 0) junta todos os bloquinhos com um Gather e mostra o resultado.<br>
mpiexec -n 8 python3 mpi_gather_criancas.py<br>
[Lider] Recebi 8 bloquinhos de tamanho 10.<br>
<br>
Do processo 0: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]<br>
Do processo 1: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]<br>
Do processo 2: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]<br>
Do processo 3: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]<br>
Do processo 4: [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]<br>
Do processo 5: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5]<br>
Do processo 6: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6]<br>
Do processo 7: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7]<br>
<br>
Vetor achatado (como no C):<br>
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]<br>
<br>
<br>
Contagem de Primos com MPI (Bag-of-Tasks)

Este exemplo conta quantos n√∫meros primos existem em 1..n usando distribui√ß√£o din√¢mica de trabalho (bag-of-tasks) com MPI.

Linguagem: C (MPI)

Arquivo: mpi_primosbag.c

Padr√µes MPI: comunica√ß√£o ponto-a-ponto (MPI_Send, MPI_Recv), curingas (MPI_ANY_SOURCE, MPI_ANY_TAG), cron√¥metro (MPI_Wtime), barreira (MPI_Barrier)

Por que bag-of-tasks?

Testes de primalidade t√™m custos desiguais. Em vez de dar um bloco fixo para cada processo (risco de desbalanceamento), o mestre (rank 0) mant√©m uma ‚Äúsacola‚Äù de tarefas (intervalos de tamanho TAMANHO) e realimenta cada trabalhador quando ele termina, at√© esgotar o dom√≠nio.

Como funciona

Par√¢metros

n (linha de comando): maior inteiro a testar (1..n).

TAMANHO (define o tamanho do bloco enviado a cada trabalhador; padr√£o 500000).

Mestre (rank 0)

Marca tempo inicial com MPI_Wtime().

Envia blocos iniciais (inicio = 3, 3+TAMANHO, ‚Ä¶) para ranks 1..P-1.

Em la√ßo:

Recebe contagem parcial de qualquer trabalhador (MPI_ANY_SOURCE).

Soma em total.

Se ainda h√° trabalho (inicio ‚â§ n), envia novo bloco com tag normal (tag=1).

Caso contr√°rio, envia mensagem de t√©rmino (tag 99) para esse trabalhador.

Soma 1 pelo n√∫mero primo 2 (tratado √† parte).

Imprime contagem total e tempo de execu√ß√£o.

Trabalhadores (ranks ‚â• 1)

Em la√ßo:

Recebem inicio do mestre; se tag=99, encerram.

Sen√£o, contam primos √≠mpares no intervalo [inicio, min(inicio+TAMANHO, n)) (passo 2).

Enviam cont de volta ao mestre.

Teste de primalidade: fun√ß√£o primo(i) verifica divisores √≠mpares at√© ‚åä‚àöi‚åã. O 2 √© somado separadamente pelo mestre.

Chamadas MPI (n√∫cleo)
// Mestre: distribui√ß√£o din√¢mica
MPI_Send(&inicio, 1, MPI_INT, dest, tag, MPI_COMM_WORLD);
MPI_Recv(&cont, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &estado);

// Cronometragem
t_inicial = MPI_Wtime();
// ... trabalho ...
t_final = MPI_Wtime();

// Sincroniza√ß√£o final
MPI_Barrier(MPI_COMM_WORLD);

Compila√ß√£o e execu√ß√£o
# Compilar (link com libm por causa de sqrt)
mpicc -O2 -lm mpi_primosbag.c -o mpi_primosbag

# Executar (m√≠nimo 2 processos: 1 mestre + 1 trabalhador)
mpiexec -n 4 ./mpi_primosbag 100000000


Sa√≠da (exemplo):

Quant. de primos entre 1 e 100000000: 5761455
Tempo de execucao: 3.842


Os valores dependem da m√°quina e do TAMANHO.

Dicas de uso

Processos: P ‚â• 2 (1 mestre + ‚â•1 trabalhador).

TAMANHO:

Grande: menos mensagens, mas pode desbalancear.

Pequeno: melhor balanceamento, por√©m mais overhead de comunica√ß√£o.

Acur√°cia:

S√≥ √≠mpares s√£o testados (i += 2); o 2 entra no final.

Cada trabalhador limita seu intervalo a i < n.

Melhorias poss√≠veis:

Peneira segmentada (Sieve) para acelerar.

Non-blocking (MPI_Isend/Irecv + MPI_Wait*) para sobrepor computa√ß√£o/comunica√ß√£o.

OpenMP dentro do processo para paralelizar a contagem do bloco.

Pseudoc√≥digo
mestre:
  t0 = Wtime()
  distribuir blocos iniciais a ranks 1..P-1
  enquanto stop < P-1:
    recv(cont, de qualquer)
    total += cont
    se ainda h√° trabalho:
       send(novo_inicio, tag=1, para esse rank)
    senao:
       send(_, tag=99, para esse rank)  // sinaliza t√©rmino
       stop++
  total += 1  // inclui o primo 2
  print(total, Wtime()-t0)

trabalhador:
  repita:
    recv(inicio, tag)
    se tag == 99: break
    cont = contar_primos_√≠mpares(inicio .. inicio+TAMANHO)
    send(cont, para mestre)

Trecho do teste de primalidade (C)
int primo (int n) {
  if (n < 2) return 0;
  if (n == 2) return 1;
  if (n % 2 == 0) return 0;
  for (int i = 3; i <= (int)(sqrt(n)); i += 2) {
    if (n % i == 0) return 0;
  }
  return 1;
}
<br>
mpiexec -n 4 python mpi_primos_bag.py 100000000<br>
<br>
mpiexec -n 16 python3 mpi_primos_bag.py 10000<br>
Quant. de primos entre 1 e 10000: 1229<br>
Tempo de execucao: 0.009 s<br>
<br>

<br>
<br>
Python + mpi4py do seu mpi_allgather.c, explicada ‚Äúpara crian√ßas‚Äù. <br>
Pense que cada crian√ßa (processo) tem 1024 bolinhas com n√∫meros entre 0 e 1. <br>
Cada crian√ßa calcula a sua m√©dia e depois todas as crian√ßas trocam suas m√©dias entre si <br>
usando o Allgather (tipo ‚Äútodo mundo conta sua m√©dia para todo mundo‚Äù).<br>
No fim, cada uma consegue calcular a m√©dia da turma.<br>
<br>
<br>
MPI Com fun√ßoes, para identificar vers√µes e validar tempo<br>
<br>
pense no programa como um ‚Äúcart√£o de identidade do time MPI‚Äù ‚Äî cada jogadora (processo) diz quem √© (rank), quantas jogadoras existem (size), em qual ‚Äúcomputador-quadra‚Äù est√° jogando e qual √© a vers√£o do uniforme (vers√£o do MPI).<br>
<br>
Tamb√©m medimos quanto tempo a checagem levou e qual √© a precis√£o do rel√≥gio do juiz (Wtick).<br>
<br>
mpiexec -n 8 python3 mpi_funcoes_criancas.py<br>
Versao do MPI = 2 Subversao = 0<br>
Numero de tarefas = 8  Meu ranque = 3  Executando em DESKTOP-QB4IQQB<br>
Foram gastos 0.000355 segundos com precisao de 6.984e-08 segundos<br>
Versao do MPI = 2 Subversao = 0<br>
Numero de tarefas = 8  Meu ranque = 0  Executando em DESKTOP-QB4IQQB<br>
Foram gastos 0.000345 segundos com precisao de 6.984e-08 segundos<br>
Versao do MPI = 2 Subversao = 0<br>
Numero de tarefas = 8  Meu ranque = 4  Executando em DESKTOP-QB4IQQB<br>
Foram gastos 0.000291 segundos com precisao de 6.984e-08 segundos<br>
Versao do MPI = 2 Subversao = 0<br>
Numero de tarefas = 8  Meu ranque = 5  Executando em DESKTOP-QB4IQQB<br>
Foram gastos 0.000405 segundos com precisao de 6.984e-08 segundos<br>
Versao do MPI = 2 Subversao = 0<br>
Numero de tarefas = 8  Meu ranque = 6  Executando em DESKTOP-QB4IQQB<br>
Foram gastos 0.000339 segundos com precisao de 6.984e-08 segundos<br>
Versao do MPI = 2 Subversao = 0<br>
Numero de tarefas = 8  Meu ranque = 2  Executando em DESKTOP-QB4IQQB<br>
Foram gastos 0.000249 segundos com precisao de 6.984e-08 segundos<br>
Versao do MPI = 2 Subversao = 0<br>
Numero de tarefas = 8  Meu ranque = 7  Executando em DESKTOP-QB4IQQB<br>
Foram gastos 0.000340 segundos com precisao de 6.984e-08 segundos<br>
Versao do MPI = 2 Subversao = 0<br>
Numero de tarefas = 8  Meu ranque = 1  Executando em DESKTOP-QB4IQQB<br>
Foram gastos 0.000252 segundos com precisao de 6.984e-08 segundos<br>
<br>
vers√£o com GATHER para agrupar tudo <br>
mpiexec -n 8 python3 mpi_funcoes_criancas_gather.py<br>
Versao do MPI = 2  Subversao = 0<br>
<br>
Resumo dos processos (ordenado por rank):<br>
rank | size | maquina                      | tempo (s) | Wtick (s)<br>
-----+------+-------------------------------+-----------+----------<br>
   0 |    8 | DESKTOP-QB4IQQB               |  0.000341 | 6.984e-08<br>
   1 |    8 | DESKTOP-QB4IQQB               |  0.000183 | 6.984e-08<br>
   2 |    8 | DESKTOP-QB4IQQB               |  0.000340 | 6.984e-08<br>
   3 |    8 | DESKTOP-QB4IQQB               |  0.000192 | 6.984e-08<br>
   4 |    8 | DESKTOP-QB4IQQB               |  0.000214 | 6.984e-08<br>
   5 |    8 | DESKTOP-QB4IQQB               |  0.000377 | 6.984e-08<br>
   6 |    8 | DESKTOP-QB4IQQB               |  0.000234 | 6.984e-08<br>
   7 |    8 | DESKTOP-QB4IQQB               |  0.000335 | 6.984e-08<br>
<br>
<br>
mpiexec -n 4 python3 mpi_medias_allgather.py <br>
[Crianca 1] Soma local = 525.791824 | Media local = 0.513469<br>
[Crianca 1] Media da turma (calculada aqui) = 0.502864<br>
[Crianca 3] Soma local = 507.466116 | Media local = 0.495572<br>
[Crianca 3] Media da turma (calculada aqui) = 0.502864<br>
[Crianca 2] Soma local = 514.933010 | Media local = 0.502864<br>
[Crianca 2] Media da turma (calculada aqui) = 0.502864<br>
[Crianca 0] Soma local = 511.540861 | Media local = 0.499552<br>
[Crianca 0] Media da turma (calculada aqui) = 0.502864<br>
<br>
<br>
vers√£o em Python + mpi4py do seu allreduce, explicada ‚Äúpara em problemas simples‚Äù - uso academico. <br><br>
Pense que cada crian√ßa (processo) tem 1024 bolinhas com n√∫meros entre 0 e 1. <br>
Cada uma calcula sua soma/m√©dia local. Depois, com Allreduce, todas somam tudo juntas (como se toda a turma colocasse as bolinhas numa mesa invis√≠vel e somasse de uma vez). <br>
Em seguida, cada uma calcula o quanto suas bolinhas est√£o longe da m√©dia da turma (diferen√ßa ao quadrado) e novamente usa Allreduce para achar o desvio padr√£o da turma.
<br>
<br>
mpiexec -n 6 python3 mpi_media_desvio_allreduce.py<br>
[Crianca 0] Soma local = 527.198582 | Media local = 0.514842<br>
[Crianca 0] Media da turma = 0.505219 | Desvio padrao = 0.288570<br>
[Crianca 1] Soma local = 514.481876 | Media local = 0.502424<br>
[Crianca 1] Media da turma = 0.505219 | Desvio padrao = 0.288570<br>
[Crianca 3] Soma local = 521.256441 | Media local = 0.509039<br>
[Crianca 3] Media da turma = 0.505219 | Desvio padrao = 0.288570<br>
[Crianca 2] Soma local = 522.115765 | Media local = 0.509879<br>
[Crianca 2] Media da turma = 0.505219 | Desvio padrao = 0.288570<br>
[Crianca 5] Soma local = 515.700250 | Media local = 0.503614<br>
[Crianca 5] Media da turma = 0.505219 | Desvio padrao = 0.288570<br>
[Crianca 4] Soma local = 503.313458 | Media local = 0.491517<br>
[Crianca 4] Media da turma = 0.505219 | Desvio padrao = 0.288570<br>
<br>
BARRIER<br>
Aqui vai a vers√£o em Python + mpi4py do seu barrier, explicada de forma academica<br>
<br>
Pense que a barreira √© um port√£o no parquinho: ningu√©m pode passar at√© todas as crian√ßas chegarem. <br>
A crian√ßa 0 chega atrasada (espera o Enter), as outras ficam esperando no port√£o. <br>
Quando todo mundo chega, o port√£o abre e todas passam juntas.<br>
<br>
mpiexec -n 4 python mpi_barreira_criancas.py <br>
mpiexec -n 8 python3 mpi_barreira_criancas.py <br>
[Crianca 7] Cheguei na barreira e estou esperando a Crianca 0...<br>
[Crianca 4] Cheguei na barreira e estou esperando a Crianca 0...<br>
[Crianca 3] Cheguei na barreira e estou esperando a Crianca 0...<br>
[Crianca 2] Cheguei na barreira e estou esperando a Crianca 0...<br>
[Crianca 5] Cheguei na barreira e estou esperando a Crianca 0...<br>
[Crianca 6] Cheguei na barreira e estou esperando a Crianca 0...<br>
[Crianca 1] Cheguei na barreira e estou esperando a Crianca 0...<br>
[Crianca 0] Estou atrasada para a barreira! (segurando o portao)<br>
Pressione Enter para eu chegar (ou aguarde 3s)... <br>
[Crianca 0] Passei da barreira! Sou 0 de 8.<br>
[Crianca 4] Passei da barreira! Sou 4 de 8.<br>
[Crianca 2] Passei da barreira! Sou 2 de 8.<br>
[Crianca 6] Passei da barreira! Sou 6 de 8.<br>
[Crianca 1] Passei da barreira! Sou 1 de 8.<br>
[Crianca 5] Passei da barreira! Sou 5 de 8.<br>
[Crianca 3] Passei da barreira! Sou 3 de 8.<br>
[Crianca 7] Passei da barreira! Sou 7 de 8.<br>
<br>
BROADCAST<br>
Aqui vai a vers√£o em Python + mpi4py do seu bcast, explicada de forma academica<br>
<br>
Pense que o broadcast √© como o l√≠der da turma gritando um n√∫mero no p√°tio: <br>
o L√≠der (Crian√ßa 0) fala uma vez e todas as crian√ßas escutam e passam a saber o mesmo n√∫mero.<br>
<br>
mpiexec -n 8 python3 mpi_broadcast_criancas.py<br>
[Lider] Entre um valor inteiro:<br>
4<br>
[Crianca 0] Recebi o valor do Lider: 4<br>
[Crianca 1] Recebi o valor do Lider: 4<br>
[Crianca 2] Recebi o valor do Lider: 4<br>
[Crianca 4] Recebi o valor do Lider: 4<br>
[Crianca 5] Recebi o valor do Lider: 4<br>
[Crianca 3] Recebi o valor do Lider: 4<br>
[Crianca 6] Recebi o valor do Lider: 4<br>
[Crianca 7] Recebi o valor do Lider: 4<br>
<br>
BSEND

Vers√£o usando bsend , explicada para uso academico.

Imagine que cada crian√ßa tem um cart√£o com 4 n√∫meros iguais (ex.: [6,6,6,6]). <br>
Em cada rodada, ela troca cart√µes com um ‚Äúvizinho‚Äù <br>
(que vai ficando cada vez mais distante: 1 passo, depois 2, depois 4‚Ä¶). <br>
Ap√≥s cada troca, cada crian√ßa fica com o maior n√∫mero em cada posi√ß√£o. <br>
No fim, todas sabem o maior de cada posi√ß√£o entre todas as crian√ßas.<br>
Aqui usamos Bsend (envio bufferizado): √© como colocar a carta numa caixinha de correio (buffer) antes de enviar.<br>
<br>
mpiexec -n 4 python mpi_bsend_criancas.py<br>
<br>
mpiexec -n 4 python3 mpi_bsend_criancas.py<br>
[Rank 3] Valor inicial = 12 | Cartao final (maximos) = [12, 12, 12, 12]<br>
[Rank 1] Valor inicial = 4 | Cartao final (maximos) = [12, 12, 12, 12]<br>
[Rank 2] Valor inicial = 8 | Cartao final (maximos) = [12, 12, 12, 12]<br>
[INFO] Buffer de Bsend anexado com 55 bytes.<br>
[Rank 0] Valor inicial = 0 | Cartao final (maximos) = [12, 12, 12, 12]<br>
<br>
<br>
Usando tipo dtype NumPy equivalente ao struct do C , com MPI BroadCast e Gather para juntar os valores
Ideia<br>
<br>
Imagine que cada ‚Äúpart√≠cula‚Äù √© uma fichinha com 6 campos:<br>
<br>
4 n√∫meros de ponto flutuante: x, y, z, velocidade<br>
<br>
2 n√∫meros inteiros: n, tipo<br>
<br>
O L√≠der (rank 0) preenche um pacote com 25 fichinhas e, com um broadcast, manda o mesmo pacote para todas as crian√ßas (processos).<br>
Usamos um tipo MPI derivado para dizer ao MPI exatamente como essa fichinha √© organizada na mem√≥ria.<br>
o rank 0 cria as fichas e faz broadcast.<br>
cada rank aplica um deslocamento simples nas suas fichas.<br>
fazemos um gather de volta no rank 0, que desenha um gr√°fico (x vs y) com um grupo por processo.<br>
se voc√™ estiver num ambiente sem janela gr√°fica, ser√° salvo um arquivo particulas.png na pasta atual <br>
<br>
MPI group include<BR>
Dois grupos (metade/metade), novo comunicador para cada grupo e Allreduce (soma) dentro do grupo.<BR>
Ideia: temos 8 crian√ßas (processos) numeradas de 0 a 7.<br>
Formamos dois times:<br>
Time A = [0, 1, 2, 3]<br>
Time B = [4, 5, 6, 7]<br>
Cada crian√ßa entra no seu time (grupo) e ganha um novo n√∫mero de camiseta (o novo rank) dentro do novo p√°tio (comunicador) do time.<br>
Dentro de cada p√°tio, fazemos uma soma coletiva dos n√∫meros das camisetas (Allreduce com soma).<br>
Cada crian√ßa mostra: rank antigo, novo rank e o resultado da soma do seu time.<br>
mpiexec -n 4 python3 mpi_grupos_meia_turma.py<br>
[Time Cima] rank antigo =  2 | novo rank =  0 | soma do time = 5<br>
[Time Cima] rank antigo =  3 | novo rank =  1 | soma do time = 5<br>
[Time Baixo] rank antigo =  0 | novo rank =  0 | soma do time = 1<br>
[Time Baixo] rank antigo =  1 | novo rank =  1 | soma do time = 1<br>
<br>
<br>
mpiexec -n 10 python3 mpi_grupos_meia_turma_stats.py<br>
[Time Baixo] rank antigo =  2 -> novo rank =  2<br>
[Time Cima] rank antigo =  7 -> novo rank =  2<br>
[Time Baixo] rank antigo =  1 -> novo rank =  1<br>
[Time Cima] rank antigo =  9 -> novo rank =  4<br>
[Time Baixo] rank antigo =  0 -> novo rank =  0<br>
<br>
=== Resumo do Time Baixo === <br>
Membros (ranks antigos): [0, 1, 2, 3, 4] <br>
Tamanho do time: 5<br>
Soma dos ranks antigos: 10<br>
M√ödia dos ranks antigos: 2.000<br>
============================== <br>
<br>
[Time Baixo] rank antigo =  3 -> novo rank =  3<br>
[Time Cima] rank antigo =  8 -> novo rank =  3<br>
[Time Cima] rank antigo =  5 -> novo rank =  0<br>
 <br>
=== Resumo do Time Cima ===<br>
Membros (ranks antigos): [5, 6, 7, 8, 9]<br>
Tamanho do time: 5<br>
Soma dos ranks antigos: 35<br>
M√ödia dos ranks antigos: 7.000<br>
==============================<br>
<br>
[Time Baixo] rank antigo =  4 -> novo rank =  4<br>
[Time Cima] rank antigo =  6 -> novo rank =  1<br>
<br>
<BR>
MPI_GROUP e MPI COMM (comunicadores)
grupos e comunicadores ‚Äî explicada de forma academica .<BR>
<BR>
Ideia: imagine 10 crian√ßas numeradas de 0 a 9.<BR>
<BR>
Montamos dois times com algumas crian√ßas:<BR>
<BR>
Time A = [0, 2, 3, 4, 5, 8]<BR>
<BR>
Time B = [9, 4, 0, 2, 8]<BR>
<BR>
Depois fazemos a uni√£o dos times (sem repetir crian√ßas) e criamos um novo p√°tio (comunicador) s√≥ para quem est√° na uni√£o.<BR>
<BR>
Dentro desse novo p√°tio, cada crian√ßa ganha um novo n√∫mero de camiseta (o novo rank).<BR>
<BR>
Quem n√£o est√° no novo time n√£o entra no p√°tio novo.<BR>
<BR>
mpiexec -n 10 python3 mpi_grupos_criancas.py<BR>
ranque antigo = 4 | novo ranque = 3<BR>
ranque antigo = 3 | novo ranque = 2<BR>
ranque antigo = 2 | novo ranque = 1<BR>
ranque antigo = 5 | novo ranque = 4<BR>
ranque antigo = 8 | novo ranque = 5<BR>
ranque antigo = 9 | novo ranque = 6<BR>
ranque antigo = 0 | novo ranque = 0<BR>
<BR>
(os ranks 1, 6 e 7 n√£o aparecem porque n√£o entraram na uni√£o dos grupos)<BR>
<BR>
Dica did√°tica: o novo rank √© como o novo n√∫mero de camiseta dentro do novo time (comunicador). Fora desse time, o n√∫mero ‚Äúantigo‚Äù (rank do mundo) continua o mesmo.<BR>
<BR>
<BR>
<BR>
mpi_trapezio, com a mesma l√≥gica (envios/recebimentos expl√≠citos) e uma explica√ß√£o academica.<BR>
usando send e receive para distribuir os calculos
<BR>
Pense assim: queremos medir a √°rea sob a curva da fun√ß√£o exp(x) entre a=0 e b=1. Dividimos em muitos ‚Äútrapezinhos‚Äù. Cada crian√ßa (processo) soma alguns trapezinhos, e o rank 0 junta tudo no final.<BR>
<BR>
mpiexec -n 4 python3 mpi_trapezio_criancas.py<BR>
Foram gastos 11.3 segundos<BR>
Com n = 100000000 trapezoides, a estimativa<BR>
da integral de 0.000000 ate 1.000000 = 1.718281855372<BR>
<BR>
mpiexec -n 8 python3 mpi_trapezio_criancas.py<BR>
Foram gastos 7.2 segundos<BR>
Com n = 100000000 trapezoides, a estimativa<BR>
da integral de 0.000000 ate 1.000000 = 1.718281855573<BR>
<BR>
<BR>
vers√£o curta usando coletiva (Reduce).<BR>
Se quiser que todos os processos recebam o resultado, troque reduce por allreduce.<BR>
vers√£o de trapezio com reduce , ficou mais lenta nos meus testes <BR>
<BR>
mpiexec -n 4 python3 mpi_trapezio_coletiva.py<BR>
Foram gastos 18.2 segundos<BR>
Com n = 100000000 trapezoides, a estimativa<BR>
da integral de 0.000000 at√ö 1.000000 = 1.718281828459<BR>
<BR>
mpiexec -n 8 python3 mpi_trapezio_coletiva.py<BR>
Foram gastos 10.9 segundos<BR>
Com n = 100000000 trapezoides, a estimativa<BR>
da integral de 0.000000 at√ö 1.000000 = 1.718281828459<BR>

<br>
 mpi_alltoall_sensores.py<br>
 Objetivo acad√™mico:<br>
 - Cada processo gera 1 leitura para cada "sensor" (0..size-1) => vetor de tamanho "size".<br>
 - Usando MPI_Alltoall, o elemento j vai para o processo j.<br>
 - Assim, o processo j recebe todas as leituras do "sensor j" (uma de cada processo) e calcula a m√©dia.<br>
mpi_comm.Alltoall<br>
<br>
mpiexec -n 4  python3 all_to_all.py<br>
[Entrada]  rank 0: [0, 1, 2, 3]<br>
[Saida]    rank 0: [0, 100, 200, 300]<br>
[Sensor 0] min=0  max=300  media=150.00<br>
[Entrada]  rank 1: [100, 101, 102, 103]<br>
[Saida]    rank 1: [1, 101, 201, 301]<br>
[Sensor 1] min=1  max=301  media=151.00<br>
[Entrada]  rank 3: [300, 301, 302, 303]<br>
[Saida]    rank 3: [3, 103, 203, 303]<br>
[Sensor 3] min=3  max=303  media=153.00<br>
[Entrada]  rank 2: [200, 201, 202, 203]<br>
[Saida]    rank 2: [2, 102, 202, 302]<br>
[Sensor 2] min=2  max=302  media=152.00<br>
<br>
<br>
<br>
Dividindo os problemas para todos <br>

mpiexec -n 4 python3 is_alltoall_demo.py --n 10000 --K 1000 --debug<br>
[rank 3] recv=9926  faixas_ok=True  total_ok=True borders_ok=True  range=(750,1000)<br>
[rank 2] recv=10159  faixas_ok=True  total_ok=True borders_ok=True  range=(500,750)<br>
[rank 1] recv=10028  faixas_ok=True  total_ok=True borders_ok=True  range=(250,500)<br>
[DEBUG] global_counts = [9887, 10028, 10159, 9926]<br>
[rank 0] recv=9887  faixas_ok=True  total_ok=True borders_ok=True  range=(0,250)<br>
[OK=True] tempo total = 0.032947 s  (P=4, N=10000, K=1000)<br>
<br>
<br>
<br>
Exemplo Acad√™mico: Multigrid (Ciclo-V) 2D para Poisson<br>
<br>
Este exemplo resolve a equa√ß√£o de Poisson -‚àá¬≤ u = f em [0,1] √ó [0,1] com condi√ß√£o de Dirichlet zero (bordas u = 0).
A implementa√ß√£o usa mpi4py com comunicador cartesiano e faz troca de halos (ghost cells) entre vizinhos a cada etapa de suaviza√ß√£o.<br>
<br>
O c√≥digo est√° bem comentado e inclui um bloco de documenta√ß√£o no topo ‚Äî pronto para entrar na sua doc.<br>

Resumo did√°tico<br>
<br>
Dom√≠nio e malha: o dom√≠nio [0,1] √ó [0,1] √© discretizado em uma malha uniforme Nx √ó Ny.<br>
<br>
Particionamento paralelo: a malha √© particionada em um grid 2D de processos (cartesiano); cada processo resolve seu subdom√≠nio com c√©lulas de halo.<br>
<br>
Ciclo-V (V-cycle):<br>
<br>
Pr√©-suaviza√ß√£o (relaxa√ß√£o, p.ex. Jacobi/GS)<br>
<br>
Restri√ß√£o do res√≠duo para a malha mais grossa<br>
<br>
Solu√ß√£o aproximada no n√≠vel grosseiro<br>
<br>
Prolongamento da corre√ß√£o para a malha fina<br>
<br>
P√≥s-suaviza√ß√£o<br>
<br>
Comunica√ß√£o MPI:<br>
<br>
Troca de fronteiras (vizinho-a-vizinho) com MPI_Sendrecv (ou MPI_Isend/Irecv + MPI_Wait*)<br>
<br>
Redu√ß√µes globais (norma do res√≠duo, produtos internos) com MPI_Allreduce<br>
<br>
Objetivo pedag√≥gico: mostrar como o Multigrid reduz erro em v√°rios comprimentos de onda e onde o MPI entra (halos entre vizinhos e redu√ß√µes globais).<br>
<br>
mpiexec -n 4 python3 mpi_multigrid_vcycle.py --Nx 128 --Ny 128 --cycles 5
<br>
<br>
<br>
exemplo acad√™mico EP (Embarrassingly Parallel) ‚Äì Monte Carlo em Python + mpi4py.
Cada processo gera pontos aleat√≥rios de forma independente e s√≥ se comunica no final usando Reduce ou Allreduce para somar os acertos.<br>
<br><br>
mpiexec -n 4 python ep_monte_carlo.py --samples-per-rank 2000000 --op allreduce<br>
mpiexec -n 4 python ep_monte_carlo.py --samples-per-rank 2000000 --op reduce<br>
mpiexec -n 4 python ep_monte_carlo.py --samples-per-rank 2000000 --op both<br>
<br>
O que esse exemplo ilustra

EP (Embarassingly Parallel): cada rank trabalha 100% independente (gera amostras e conta acertos).

Reduce (root-only): apenas o rank 0 recebe as somas globais.

Allreduce (broadcast do resultado): todos recebem as somas globais.
<br>
mpiexec -n 4 python3 ep_monte_carlo.py --samples-per-rank 2000000 --op allreduce
<br>
[EP Monte Carlo] P=4 | samples_per_rank=2,000,000 | total=8,000,000 <br>
Tempo total: 0.154422 s <br>
  Allreduce -> pi ~= 3.140547000000 | erro=1.046e-03 <br>
<br>
<br>
Codigo em python para uso academico para testar CG - Ax=b esparso (SpMV) Por linhas/blocos Halo exchange + Allreduce Isend/Irecv/Sendrecv, Allreduce<br>

<br>
CG (Conjugate Gradient) distribu√≠do com SpMV esparso, halo exchange e redu√ß√µes globais

Este material explica ‚Äî de forma did√°tica e no estilo de ‚ÄúREADME de GitHub‚Äù ‚Äî como implementar e entender um resolvedor Conjugate Gradient (CG) paralelo para o sistema linear esparso

ùê¥
‚Äâ
ùë•
=
ùëè
,
Ax=b,

onde A √© sim√©trica definida positiva (SPD). No exemplo acad√™mico, A vem do Laplaciano 2D com condi√ß√µes de contorno de Dirichlet (u=0 nas bordas), discretizado por stencil de 5 pontos. O foco √© mostrar:

SpMV (produto matriz‚Äìvetor) matriceless (sem montar A): usamos diretamente o stencil.

Decomposi√ß√£o por linhas/blocos (1D): cada processo guarda um bloco cont√≠guo de linhas do dom√≠nio global.

Troca de halos (ghost rows) entre processos vizinhos para viabilizar o stencil.

Comunica√ß√£o ponto-a-ponto com Sendrecv (bloqueante) ou Isend/Irecv (n√£o-bloqueante).

Redu√ß√µes globais com Allreduce (produtos internos e norma do res√≠duo).

1) Problema de refer√™ncia

Dom√≠nio: 
[
0
,
1
]
√ó
[
0
,
1
]
[0,1]√ó[0,1], malha uniforme 
ùëÅ
ùë•
√ó
ùëÅ
ùë¶
N
x
	‚Äã

√óN
y
	‚Äã

.

Operador: 
‚àí
‚àá
2
ùë¢
=
ùëì
‚àí‚àá
2
u=f com 
ùë¢
=
0
u=0 nas bordas.

Discretiza√ß√£o 5-pontos em cada c√©lula interior:

(
ùê¥
ùë¢
)
ùëñ
,
ùëó
‚ÄÖ‚Ää
=
‚ÄÖ‚Ää
4
‚Äâ
ùë¢
ùëñ
,
ùëó
‚àí
(
ùë¢
ùëñ
‚àí
1
,
ùëó
+
ùë¢
ùëñ
+
1
,
ùëó
+
ùë¢
ùëñ
,
ùëó
‚àí
1
+
ùë¢
ùëñ
,
ùëó
+
1
)
(Au)
i,j
	‚Äã

=4u
i,j
	‚Äã

‚àí(u
i‚àí1,j
	‚Äã

+u
i+1,j
	‚Äã

+u
i,j‚àí1
	‚Äã

+u
i,j+1
	‚Äã

)

Observa√ß√£o: frequentemente absorvemos 
‚Ñé
‚àí
2
h
‚àí2
 no lado direito (
ùëè
=
‚Ñé
2
ùëì
b=h
2
f) para simplificar a nota√ß√£o do SpMV.

2) Por que SpMV matriceless?

Em malhas regulares, A tem estrutura local (stencil). Montar uma matriz esparsa global √© desnecess√°rio e caro. Em vez disso, computamos 
(
ùê¥
ùë£
)
(Av) no ato, apenas com acessos aos vizinhos de cada n√≥. Isso reduz mem√≥ria e melhora cache.

3) Decomposi√ß√£o por linhas/blocos

Dividimos o dom√≠nio global por faixas horizontais de linhas (decomposi√ß√£o 1D):

Global (Ny x Nx)
+--------- Rank 0 ---------+
| linhas 0 .. y_end_0      |
+--------- Rank 1 ---------+
| linhas y0+1 .. y_end_1   |
+--------- Rank 2 ---------+
| ...                      |
+--------- Rank P-1 -------+
| ...                      |
+--------------------------+


Cada processo armazena seu bloco com duas linhas fantasma (uma no topo, outra na base). Essas linhas formam o halo, preenchido com dados reais vindos dos vizinhos. As colunas laterais usam Dirichlet 0 (sem troca lateral neste particionamento 1D).

4) Halo exchange (linhas fantasmas)

Antes de aplicar o stencil, precisamos dos valores da linha de cima do vizinho de cima, e da linha de baixo do vizinho de baixo.

Op√ß√£o A ‚Äî Sendrecv (sim√©trico, bloqueante)

Passo 1: envia a 1¬™ linha interior para cima e recebe do vizinho de baixo (preenche ghost sul).

Passo 2: envia a √∫ltima linha interior para baixo e recebe do vizinho de cima (preenche ghost norte).

Op√ß√£o B ‚Äî Isend/Irecv (n√£o-bloqueante)

Posta dois Irecv (de cima e baixo).

Posta dois Isend (para cima e baixo).

Finaliza com Waitall.

Sem deadlock: use tags consistentes e sempre o mesmo padr√£o de envio/recebimento em todos os processos.

5) Conjugate Gradient (CG) com redu√ß√µes globais

O CG cl√°ssico (para SPD) itera:

ùëü
0
=
ùëè
‚àí
ùê¥
ùë•
0
r
0
	‚Äã

=b‚àíAx
0
	‚Äã

, 
ùëù
0
=
ùëü
0
p
0
	‚Äã

=r
0
	‚Äã


Para 
ùëò
=
0
,
1
,
2
,
‚Ä¶
k=0,1,2,‚Ä¶ at√© convergir:

SpMV: 
ùê¥
ùëù
ùëò
Ap
k
	‚Äã

 ‚Üí requer halo exchange.

ùõº
ùëò
=
ùëü
ùëò
ùëá
ùëü
ùëò
ùëù
ùëò
ùëá
ùê¥
ùëù
ùëò
Œ±
k
	‚Äã

=
p
k
T
	‚Äã

Ap
k
	‚Äã

r
k
T
	‚Äã

r
k
	‚Äã

	‚Äã

 ‚ÄÉ(2 produtos internos)
‚Üí Allreduce(SUM) para cada dot product.

ùë•
ùëò
+
1
=
ùë•
ùëò
+
ùõº
ùëò
ùëù
ùëò
x
k+1
	‚Äã

=x
k
	‚Äã

+Œ±
k
	‚Äã

p
k
	‚Äã


ùëü
ùëò
+
1
=
ùëü
ùëò
‚àí
ùõº
ùëò
ùê¥
ùëù
ùëò
r
k+1
	‚Äã

=r
k
	‚Äã

‚àíŒ±
k
	‚Äã

Ap
k
	‚Äã


Crit√©rio de parada: 
‚à•
ùëü
ùëò
+
1
‚à•
2
/
‚à•
ùëü
0
‚à•
2
<
tol
‚à•r
k+1
	‚Äã

‚à•
2
	‚Äã

/‚à•r
0
	‚Äã

‚à•
2
	‚Äã

<tol
‚Üí Allreduce(SUM) para norma global.

ùõΩ
ùëò
=
ùëü
ùëò
+
1
ùëá
ùëü
ùëò
+
1
ùëü
ùëò
ùëá
ùëü
ùëò
Œ≤
k
	‚Äã

=
r
k
T
	‚Äã

r
k
	‚Äã

r
k+1
T
	‚Äã

r
k+1
	‚Äã

	‚Äã


ùëù
ùëò
+
1
=
ùëü
ùëò
+
1
+
ùõΩ
ùëò
ùëù
ùëò
p
k+1
	‚Äã

=r
k+1
	‚Äã

+Œ≤
k
	‚Äã

p
k
	‚Äã


Onde entra comunica√ß√£o coletiva?

Allreduce para:

ùëü
ùëò
ùëá
ùëü
ùëò
r
k
T
	‚Äã

r
k
	‚Äã

 (norma global do res√≠duo)

ùëù
ùëò
ùëá
ùê¥
ùëù
ùëò
p
k
T
	‚Äã

Ap
k
	‚Äã

 (produto interno para 
ùõº
Œ±)

Pontos de sincroniza√ß√£o do m√©todo.

6) Esqueleto do algoritmo (pseudoc√≥digo MPI)
Particiona Ny entre P ranks ‚Üí cada um fica com ny_local linhas (+ halos)

x = 0
b_int = h^2          # interior do bloco local (Dirichlet 0 nas bordas globais)
r = b - A*x = b
p = r
rr = Allreduce( dot(r, r), SUM )
rr0 = rr

for k = 1..max_iters:
    # SpMV distribu√≠do
    HALO_EXCHANGE(p)           # Sendrecv OU Isend/Irecv + Waitall
    Ap = A * p                 # stencil 5-pontos (somente interior)

    pAp = Allreduce( dot(p, Ap), SUM )
    alpha = rr / pAp

    x = x + alpha * p
    r = r - alpha * Ap

    rr_new = Allreduce( dot(r, r), SUM )
    if sqrt(rr_new/rr0) < tol: break

    beta = rr_new / rr
    p = r + beta * p
    rr = rr_new

7) Balanceamento, custo e escalabilidade

Custo computacional (SpMV): proporcional ao n√∫mero de n√≥s locais (
‚àº
a
Àä
rea
‚àº
a
Àä
rea).

Custo de comunica√ß√£o (halo): proporcional ao per√≠metro do subdom√≠nio.
‚Üí Com decomposi√ß√£o 1D, trocamos duas linhas por itera√ß√£o (pequeno overhead).

Allreduce: traz lat√™ncia logar√≠tmica (√°rvore). √â inevit√°vel no CG; minimizar outras comunica√ß√µes ajuda.

8) Checklist de robustez (sem deadlock)

Todos os ranks chamam as mesmas coletivas na mesma ordem (Allreduce).

Em Sendrecv/Isend/Irecv:

Tags e pares (fonte/destino) batem entre vizinhos.

Comprimentos das mensagens s√£o iguais nos dois lados.

Halos laterais (decomposi√ß√£o 1D) s√£o Dirichlet 0 ‚Äî n√£o tente trocar colunas neste modelo.

Aritm√©tica: normalize com res√≠duo relativo 
‚à•
ùëü
‚à•
/
‚à•
ùëü
0
‚à•
‚à•r‚à•/‚à•r
0
	‚Äã

‚à• para um crit√©rio de parada est√°vel.

9) Como rodar (exemplo)
pip install mpi4py numpy

# 4 processos, dom√≠nio 256x256, halo por Sendrecv (bloqueante)
mpiexec -n 4 python cg_spmv_ep.py --Nx 256 --Ny 256 --mode sendrecv --max-iters 200 --tol 1e-8

# 4 processos, halo n√£o-bloqueante (Isend/Irecv)
mpiexec -n 4 python cg_spmv_ep.py --Nx 256 --Ny 256 --mode isendirecv


Valida√ß√£o r√°pida: o res√≠duo relativo deve decrescer monotonicamente e ficar < tol em poucas dezenas/centenas de itera√ß√µes (dependendo de 
ùëÅ
N).

10) Extens√µes e varia√ß√µes

Decomposi√ß√£o 2D (blocos 
ùëÉ
ùë¶
√ó
ùëÉ
ùë•
P
y
	‚Äã

√óP
x
	‚Äã

): halo em 4 dire√ß√µes.

Precondicionadores (Jacobi, SSOR, AMG) ‚Üí reduzem itera√ß√µes, mas introduzem mais comunica√ß√£o.

Overlap comunica√ß√£o‚Äìcomputa√ß√£o com Isend/Irecv (postar trocas antes do c√°lculo de linhas ‚Äúinternas‚Äù).

Arquivo de refer√™ncia

O exemplo completo (CG + SpMV + halo + Allreduce), com ambas as variantes de comunica√ß√£o, est√° implementado no script cg_spmv_ep.py neste reposit√≥rio.
<br>
mpiexec -n 8 python3 cg_spmv_ep.py --Nx 256 --Ny 256 --mode sendrecv<br>
[init] ||r||/||r0|| = 1.000e+00  (rr=1.502e-05)<br>
[it    1] ||r||/||r0|| = 7.969e+00<br>
[it   10] ||r||/||r0|| = 8.541e+00<br>
[it   20] ||r||/||r0|| = 7.979e+00<br>
[it   30] ||r||/||r0|| = 7.378e+00<br>
[it   40] ||r||/||r0|| = 6.769e+00<br>
[it   50] ||r||/||r0|| = 6.170e+00<br>
[it   60] ||r||/||r0|| = 5.588e+00<br>
[it   70] ||r||/||r0|| = 5.025e+00<br>
[it   80] ||r||/||r0|| = 4.483e+00<br>
[it   90] ||r||/||r0|| = 3.962e+00<br>
[it  100] ||r||/||r0|| = 3.462e+00<br>
[it  110] ||r||/||r0|| = 2.981e+00<br>
[it  120] ||r||/||r0|| = 2.520e+00<br>
[it  130] ||r||/||r0|| = 2.078e+00<br>
[it  140] ||r||/||r0|| = 1.653e+00<br>
[it  150] ||r||/||r0|| = 1.245e+00<br>
[it  160] ||r||/||r0|| = 8.495e-01<br>
[it  170] ||r||/||r0|| = 4.653e-01<br>
[it  180] ||r||/||r0|| = 1.467e-01<br>
[it  190] ||r||/||r0|| = 2.020e-01<br>
[it  200] ||r||/||r0|| = 1.676e-01<br>
<br>
[Resumo] modo=sendrecv  P=8  Nx=256 Ny=256<br>
Convergiu (||r||/||r0|| = 1.676e-01) em 200 iteracoes. Tempo total: 0.363087 s<br>
<br>

Determinismo: sementes independentes por rank (seed + rank*1_000_003).

Escalabilidade: comunica√ß√£o m√≠nima (somente 2 inteiros por rank no final).

Mem√≥ria controlada: gera√ß√£o de amostras por chunks.

